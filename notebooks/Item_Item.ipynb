{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5abf355-b61a-4c4a-8222-8f401ed24581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads basket data, sets efficient data types, and creates mappings.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    df_baskets = pd.read_csv(file_path)\n",
    "    df_baskets['order_id'] = df_baskets['order_id'].astype('category')\n",
    "    df_baskets['product_id'] = df_baskets['product_id'].astype('category')\n",
    "    \n",
    "    product_ids = df_baskets['product_id'].cat.categories\n",
    "    product_to_index = {product: i for i, product in enumerate(product_ids)}\n",
    "    index_to_product = {i: product for product, i in product_to_index.items()}\n",
    "    \n",
    "    print(\"Data loaded and prepared successfully.\")\n",
    "    return df_baskets, product_to_index, index_to_product\n",
    "\n",
    "def build_item_order_matrix(df_baskets):\n",
    "    \"\"\"\n",
    "    Builds a sparse item-order matrix (X) where X[i, j] = 1 if item i is in order j.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 2: Building the item-order sparse matrix...\")\n",
    "    n_products = len(df_baskets['product_id'].cat.categories)\n",
    "    n_orders = len(df_baskets['order_id'].cat.categories)\n",
    "    \n",
    "    product_codes = df_baskets['product_id'].cat.codes.values\n",
    "    order_codes = df_baskets['order_id'].cat.codes.values\n",
    "    data = np.ones(len(df_baskets))\n",
    "    \n",
    "    item_order_matrix = csr_matrix((data, (product_codes, order_codes)), shape=(n_products, n_orders))\n",
    "    \n",
    "    print(\"Item-order matrix built successfully.\")\n",
    "    return item_order_matrix\n",
    "\n",
    "def compute_topk_neighbors(item_order_matrix, index_to_product, K=10):\n",
    "    \"\"\"\n",
    "    Computes Top-K neighbors for each item using the efficient row-wise method.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 3 & 4: Computing Top-K neighbors row-by-row...\")\n",
    "    start_time = time.time()\n",
    "    n_products = item_order_matrix.shape[0]\n",
    "    top_k_similar_items = {}\n",
    "    freq = np.asarray(item_order_matrix.sum(axis=1)).ravel()\n",
    "    sqrt_freq = np.sqrt(np.clip(freq, 1e-12, None))\n",
    "\n",
    "    for i in range(n_products):\n",
    "        product_id = index_to_product[i]\n",
    "        co_row = item_order_matrix.getrow(i).dot(item_order_matrix.T)\n",
    "        neighbor_indices = co_row.indices\n",
    "        co_occurrence_values = co_row.data\n",
    "        denominator = sqrt_freq[i] * sqrt_freq[neighbor_indices]\n",
    "        similarity_scores = co_occurrence_values / np.clip(denominator, 1e-12, None)\n",
    "        \n",
    "        mask = neighbor_indices != i\n",
    "        neighbor_indices = neighbor_indices[mask]\n",
    "        similarity_scores = similarity_scores[mask]\n",
    "        \n",
    "        if len(similarity_scores) == 0:\n",
    "            top_k_similar_items[product_id] = []\n",
    "            continue\n",
    "\n",
    "        k_to_find = min(K, len(similarity_scores))\n",
    "        top_k_partition_indices = np.argpartition(similarity_scores, -k_to_find)[-k_to_find:]\n",
    "        \n",
    "        final_indices = neighbor_indices[top_k_partition_indices]\n",
    "        final_scores = similarity_scores[top_k_partition_indices]\n",
    "        sort_order = np.argsort(final_scores)[::-1]\n",
    "        \n",
    "        top_k_similar_items[product_id] = [\n",
    "            {'product_id': index_to_product[idx], 'score': score}\n",
    "            for idx, score in zip(final_indices[sort_order], final_scores[sort_order])\n",
    "        ]\n",
    "        \n",
    "    print(f\"Top-K calculation done in {time.time() - start_time:.2f} seconds.\")\n",
    "    return top_k_similar_items\n",
    "\n",
    "def save_neighbors_to_file(top_k_items, file_path):\n",
    "    \"\"\"\n",
    "    Saves the computed neighbors to a CSV file for easy reuse.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving neighbors artifact to {file_path}...\")\n",
    "    rows = []\n",
    "    for product_id, neighbors in top_k_items.items():\n",
    "        for neighbor in neighbors:\n",
    "            rows.append([product_id, neighbor['product_id'], neighbor['score']])\n",
    "    \n",
    "    df_neighbors = pd.DataFrame(rows, columns=['product_id', 'neighbor_id', 'score'])\n",
    "    df_neighbors.to_csv(file_path, index=False)\n",
    "    print(\"Artifact saved successfully.\")\n",
    "\n",
    "def get_candidate_items(user_recent_purchases, top_k_similar_items, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Generates candidate items for a user based on their recent purchases.\n",
    "    \"\"\"\n",
    "    candidate_items = defaultdict(float)\n",
    "    recent_set = set(user_recent_purchases)\n",
    "\n",
    "    for product_id in user_recent_purchases:\n",
    "        if product_id in top_k_similar_items:\n",
    "            for item in top_k_similar_items[product_id]:\n",
    "                candidate_product = item['product_id']\n",
    "                score = item['score']\n",
    "\n",
    "                if candidate_product in recent_set:\n",
    "                    continue\n",
    "                \n",
    "                if score > candidate_items[candidate_product]:\n",
    "                    candidate_items[candidate_product] = score\n",
    "\n",
    "    sorted_candidates = sorted(candidate_items.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [item[0] for item in sorted_candidates[:num_recommendations]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6182f2-3de9-4244-a77c-b49ae22c9f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preparing data...\n",
      "Data loaded and prepared successfully.\n",
      "\n",
      "Step 2: Building the item-order sparse matrix...\n",
      "Item-order matrix built successfully.\n",
      "\n",
      "Step 3 & 4: Computing Top-K neighbors row-by-row...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    # IMPORTANT: Update this path to your actual file location\n",
    "    DATA_FILE_PATH = r\"C:\\Users\\kthac\\Desktop\\Group_4419\\item item similarity\\item_item_basket.csv\"\n",
    "    NEIGHBORS_ARTIFACT_PATH = \"top_k_neighbors.csv\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df_baskets, product_to_index, index_to_product = load_and_prepare_data(DATA_FILE_PATH)\n",
    "    \n",
    "    if df_baskets is None:\n",
    "        return # Exit if data loading failed\n",
    "\n",
    "    # Build the item-order matrix\n",
    "    item_order_matrix = build_item_order_matrix(df_baskets)\n",
    "    \n",
    "    # Compute the neighbors\n",
    "    top_k_items = compute_topk_neighbors(item_order_matrix, index_to_product, K=10)\n",
    "    \n",
    "    # Save the results for future use\n",
    "    save_neighbors_to_file(top_k_items, NEIGHBORS_ARTIFACT_PATH)\n",
    "    \n",
    "    # --- Example Usage ---\n",
    "    print(\"\\nStep 5: Generating candidate items for a user...\")\n",
    "    \n",
    "    # Sample user purchases (using popular product IDs)\n",
    "    user_purchases = [13176, 21137]\n",
    "    recommended_candidates = get_candidate_items(user_purchases, top_k_items, num_recommendations=5)\n",
    "    \n",
    "    print(f\"\\nCandidate items for a user who recently bought products {user_purchases}:\")\n",
    "    print(recommended_candidates)\n",
    "    \n",
    "    # Show sample output for a specific product\n",
    "    print(\"\\nTop 3 Similar Items for a sample product (ID: 24852):\")\n",
    "    if 24852 in top_k_items and top_k_items[24852]:\n",
    "        top_3 = top_k_items[24852][:3]\n",
    "        print(top_3)\n",
    "    else:\n",
    "        print(\"Sample product ID 24852 not found or has no similar items.\")\n",
    "        \n",
    "    print(\"\\nProcess finished successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babff94-429a-493c-bdc9-5625a2ba8a85",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Initial Preparation\n",
    "This section handles data and initial memory optimization using pandas.\n",
    "pd.read_csv: Loads the basket and product count data into pandas DataFrames.\n",
    ".astype('category'): This is a key memory-saving step. Instead of storing product and order IDs as int64 or object strings, they are converted to a category dtype. Pandas internally maps each unique ID to a contiguous integer (accessible via .cat.codes), which is much more memory-efficient and provides a direct lookup for creating our sparse matrix.\n",
    "ID to Index Mapping: Dictionaries (product_to_index, index_to_product) are created to map the original product_id to its new integer index (0 to n_products-1) and back. This is necessary because the matrix will operate on these zero-based indices.\n",
    "\n",
    "# Step 2: Creating the Item-Order Sparse Matrix\n",
    "The goal here is to create a binary matrix representing which items appeared in which orders. A (num_products x num_orders) NumPy array was too large.\n",
    "scipy.sparse.csr_matrix: We use a Compressed Sparse Row (CSR) matrix because it's highly efficient for both storage and, more importantly, for the fast matrix-vector and matrix-matrix multiplications (.dot() product) required in the next step.\n",
    "csr_matrix((data, (rows, cols)), shape=...): This is the standard constructor for creating a sparse matrix from coordinate-style data.\n",
    "rows: The product indices (df_baskets['product_id'].cat.codes).\n",
    "cols: The order indices (df_baskets['order_id'].cat.codes).\n",
    "data: A list of ones, indicating the presence of an item in an order.\n",
    "\n",
    "# Step 3: Calculate Item-Item Similarity \n",
    "This is the core of the algorithm. It computes a similarity score between every pair of products.\n",
    "Co-occurrence Matrix: The line co_occurrence_matrix = item_order_matrix.dot(item_order_matrix.T) is the most critical calculation. By taking the dot product of the matrix with its transpose, we efficiently compute the co-occurrence matrix. The resulting (num_products x num_products) sparse matrix contains the raw count of how many times any two items appeared together in the same basket.\n",
    "Memory Management: A dense float matrix of the full similarity scores would be too large to fit in memory (~18 GB for 49k products). The code therefore processes the similarity calculation in chunks.\n",
    "Similarity Metric: The metric used is a normalized co-occurrence. The formula is essentially Co-occurrence(A, B) / (sqrt(Count(A)) * sqrt(Count(B))).\n",
    "np.outer: The denominator is calculated efficiently for each chunk using NumPy's outer product. This broadcasts the vectors of square-rooted product frequencies, avoiding a slow, explicit Python loop.\n",
    "lil_matrix: The results are stored in a List of Lists (LIL) matrix. lil_matrix is chosen here because it's efficient for constructing sparse matrices incrementally by changing sparsity structure (i.e., filling in the chunks row by row).\n",
    "\n",
    "# Step 4: Find Top-K Similar Items\n",
    "This section extracts the top K most similar items for each product from the final similarity matrix.\n",
    ".tocsr(): The similarity matrix is converted from LIL back to CSR format. CSR is much more efficient for row slicing, which is exactly what we're about to do in the loop.\n",
    "np.argsort(): Instead of using a computationally expensive method like sklearn.NearestNeighbors (which would require a dense distance matrix), the code uses a more direct and faster NumPy approach.\n",
    "np.argsort(row) returns the indices that would sort the array in ascending order.\n",
    "By reversing this with [::-1], we get the indices of the similarity scores from largest to smallest.\n",
    "Slicing with [:K+1] gives the indices of the top K+1 items (the item itself plus its K nearest neighbors). This is an extremely fast and memory-efficient way to perform a top-k search on each row.\n",
    "\n",
    "# Step 5: Generate Outputs (Example Usage)\n",
    "This final part is a practical demonstration of how to use the pre-computed similarities to generate recommendations.\n",
    "get_candidate_items function: This function takes a list of a user's recent purchases and returns a ranked list of recommended products.\n",
    "Candidate Aggregation: It iterates through the user's items and their corresponding similarity lists. A dictionary (candidate_items) is used to aggregate all potential recommendations.\n",
    "Score Deduplication: Using a dictionary is a clever way to handle duplicates. The line if candidate_product not in candidate_items or score > candidate_items[candidate_product] ensures that if a candidate is suggested by multiple user items, only the highest similarity score is retained.\n",
    "Final Ranking: Finally, sorted(candidate_items.items(), ...) is used with a lambda function to sort the candidates by their similarity scores in descending order before returning the top N product IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3f287-5c5f-45cd-ae93-3eb58fab49cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
